# -*- coding: utf-8 -*-
"""Extractive.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1frPjDA36O_BiEyxQpe9EbIr1kvsOGzZb
"""

pip install datasets

from datasets import load_dataset
import pandas as pd

# Load the CNN/DailyMail dataset from Hugging Face
dataset = load_dataset("abisee/cnn_dailymail", "3.0.0")

# Split dataset into train, validation, and test
train_data = dataset["train"]
validation_data = dataset["validation"]
test_data = dataset["test"]

# Convert each split to a pandas DataFrame
train_df = pd.DataFrame(train_data)
validation_df = pd.DataFrame(validation_data)
test_df = pd.DataFrame(test_data)

# Save each split as a separate CSV file
train_df.to_csv("train.csv", index=False)
validation_df.to_csv("validation.csv", index=False)
test_df.to_csv("test.csv", index=False)

print("Train, validation, and test CSV files saved successfully.")

import pandas as pd
import os

# Path to your original train dataset
train_dataset_path = "/content/train.csv"

# Path to save the new dataset with 15,000 rows
new_train_dataset_path = "content/new_train_15k.csv"

# Create the directory if it doesn't exist
os.makedirs(os.path.dirname(new_train_dataset_path), exist_ok=True)

# Load the original train dataset
train_data = pd.read_csv(train_dataset_path)

# Extract the first 15,000 rows
train_data_15k = train_data.head(15000)

# Save the new dataset to a CSV file
train_data_15k.to_csv(new_train_dataset_path, index=False)

print(f"New dataset with 15,000 rows saved to {new_train_dataset_path}")

import pandas as pd
import os

# Path to your original test dataset
test_dataset_path = "/content/test.csv"

# Path to save the new dataset with 5,000 rows
new_test_dataset_path = "content/new_test_5k.csv"

# Create the directory if it doesn't exist
os.makedirs(os.path.dirname(new_test_dataset_path), exist_ok=True)

# Load the original test dataset
test_data = pd.read_csv(test_dataset_path)

# Extract the first 5,000 rows
test_data_5k = test_data.head(5000)

# Save the new dataset to a CSV file
test_data_5k.to_csv(new_test_dataset_path, index=False)

print(f"New dataset with 5,000 rows saved to {new_test_dataset_path}")

import pandas as pd

# Load each CSV file
train_df = pd.read_csv("/content/content/new_train_15k.csv")
validation_df = pd.read_csv("validation.csv")
test_df = pd.read_csv("test.csv")

# Get the number of rows and column headers in each file
train_rows, train_columns = train_df.shape[0], train_df.columns.tolist()
validation_rows, validation_columns = validation_df.shape[0], validation_df.columns.tolist()
test_rows, test_columns = test_df.shape[0], test_df.columns.tolist()

# Print the results
print(f"Train.csv - Rows: {train_rows}, Columns: {train_columns}")
print(f"Validation.csv - Rows: {validation_rows}, Columns: {validation_columns}")
print(f"Test.csv - Rows: {test_rows}, Columns: {test_columns}")

"""preprocessing"""

!pip install pandas spacy sklearn transformers tqdm
!python -m spacy download en_core_web_sm

import pandas as pd
import spacy
from typing import Dict
import re

class RegionalNewsPreprocessor:
    def __init__(self):
        # Load SpaCy model for NER
        self.nlp = spacy.load("en_core_web_sm")
        if not "sentencizer" in self.nlp.pipe_names:
            self.nlp.add_pipe("sentencizer")

        # Define region mappings
        self.region_mapping = {
            'NA': ['United States', 'Canada', 'Mexico', 'USA', 'U.S.', 'U.S.A.'],
            'EU': ['United Kingdom', 'France', 'Germany', 'Italy', 'Spain', 'UK', 'Britain'],
            'ASIA': ['China', 'Japan', 'India', 'South Korea', 'Singapore'],
            'ME': ['Saudi Arabia', 'UAE', 'Israel', 'Iran', 'Qatar'],
            'AF': ['South Africa', 'Nigeria', 'Kenya', 'Egypt', 'Morocco'],
            'SA': ['Brazil', 'Argentina', 'Chile', 'Colombia', 'Peru'],
            'OCE': ['Australia', 'New Zealand']
        }

        # Map locations to regions
        self.location_to_region = {loc.lower(): region for region, locations in self.region_mapping.items() for loc in locations}

    def preprocess_text(self, text: str) -> str:
        """Tokenize, lowercase, remove stopwords and punctuation."""
        text = text.lower()
        text = re.sub(r'http\S+|www.\S+', '', text)  # Remove URLs
        doc = self.nlp(text)

        tokens = [
            token.text for token in doc
            if token.is_alpha and not token.is_stop
        ]
        return " ".join(tokens)

    def extract_regions(self, article: str) -> str:
        """Extracts and assigns a primary region to each article."""
        doc = self.nlp(article)
        regions = [self.location_to_region[ent.text.lower()] for ent in doc.ents if ent.label_ == "GPE" and ent.text.lower() in self.location_to_region]

        if regions:
            primary_region = max(set(regions), key=regions.count)
            return primary_region
        else:
            return "UNKNOWN"

    def preprocess_data(self, file_paths: Dict[str, str]) -> Dict[str, pd.DataFrame]:
        """Preprocesses each dataset split and returns preprocessed data."""
        processed_data = {}

        for split, path in file_paths.items():
            print(f"Processing {split} split...")
            data = pd.read_csv(path)

            # Preprocess the article text
            data['preprocessed_article'] = data['article'].apply(self.preprocess_text)

            # Extract regions
            data['primary_region'] = data['preprocessed_article'].apply(self.extract_regions)

            processed_data[split] = data

        return processed_data

#'''file_paths = {
   # "train": "/content/content/new_train_15k.csv",
  #  "validation": "/content/validation.csv",
   # "test": "/content/test.csv"
#}

# Initialize the preprocessor
#'''preprocessor = RegionalNewsPreprocessor()

# Preprocess the dataset
#processed_data = preprocessor.preprocess_data(file_paths)

# Save the processed datasets
#for split, df in processed_data.items():
    #df.to_csv(f"processed_{split}.csv", index=False)
   #print(f"{split} data saved to processed_{split}.csv")

# Save each processed split to a new CSV file
#'''for split, df in processed_data.items():
    #df.to_csv(f"processed_{split}.csv", index=False)
    #print(f"{split} data saved to processed_{split}.csv")

data = pd.read_csv("/content/content/new_train_15k.csv")  # Replace with the actual path to your dataset

import spacy

# Enable GPU for spaCy if available
#spacy.require_gpu()  # Call this before loading the model
nlp = spacy.load("en_core_web_sm")

import pandas as pd
import spacy
from collections import defaultdict

# Load dataset
data_path = "/content/content/new_train_15k.csv"
 # Replace with the path to your dataset
data = pd.read_csv(data_path)

# Define the NER function
def extract_regions(article: str) -> str:
    """Extracts and assigns a primary region to each article."""
    doc = nlp(article)
    regions = [location_to_region[ent.text.lower()] for ent in doc.ents if ent.label_ == "GPE" and ent.text.lower() in location_to_region]

    if regions:
        primary_region = max(set(regions), key=regions.count)
        return primary_region
    else:
        return "UNKNOWN"

# Enable GPU for spaCy if available (optional)
#spacy.require_gpu()  # Use this if GPU is available, otherwise remove this line
nlp = spacy.load("en_core_web_sm")

# Define region mappings
region_mapping = {
    'NA': ['United States', 'Canada', 'Mexico', 'USA', 'U.S.', 'U.S.A.'],
    'EU': ['United Kingdom', 'France', 'Germany', 'Italy', 'Spain', 'UK', 'Britain'],
    'ASIA': ['China', 'Japan', 'India', 'South Korea', 'Singapore'],
    'ME': ['Saudi Arabia', 'UAE', 'Israel', 'Iran', 'Qatar'],
    'AF': ['South Africa', 'Nigeria', 'Kenya', 'Egypt', 'Morocco'],
    'SA': ['Brazil', 'Argentina', 'Chile', 'Colombia', 'Peru'],
    'OCE': ['Australia', 'New Zealand']
}

location_to_region = {loc.lower(): region for region, locations in region_mapping.items() for loc in locations}

# Apply NER to classify regions
data['primary_region'] = data['article'].apply(extract_regions)

# Save the updated dataset
data.to_csv("processed_dataset.csv", index=False)
print("Processed dataset saved to processed_dataset.csv")

import pandas as pd
import spacy
from collections import defaultdict

# Load dataset
data_path = "/content/content/new_test_5k.csv"
 # Replace with the path to your dataset
data = pd.read_csv(data_path)

# Define the NER function
def extract_regions(article: str) -> str:
    """Extracts and assigns a primary region to each article."""
    doc = nlp(article)
    regions = [location_to_region[ent.text.lower()] for ent in doc.ents if ent.label_ == "GPE" and ent.text.lower() in location_to_region]

    if regions:
        primary_region = max(set(regions), key=regions.count)
        return primary_region
    else:
        return "UNKNOWN"

# Enable GPU for spaCy if available (optional)
#spacy.require_gpu()  # Use this if GPU is available, otherwise remove this line
nlp = spacy.load("en_core_web_sm")

# Define region mappings
region_mapping = {
    'NA': ['United States', 'Canada', 'Mexico', 'USA', 'U.S.', 'U.S.A.'],
    'EU': ['United Kingdom', 'France', 'Germany', 'Italy', 'Spain', 'UK', 'Britain'],
    'ASIA': ['China', 'Japan', 'India', 'South Korea', 'Singapore'],
    'ME': ['Saudi Arabia', 'UAE', 'Israel', 'Iran', 'Qatar'],
    'AF': ['South Africa', 'Nigeria', 'Kenya', 'Egypt', 'Morocco'],
    'SA': ['Brazil', 'Argentina', 'Chile', 'Colombia', 'Peru'],
    'OCE': ['Australia', 'New Zealand']
}

location_to_region = {loc.lower(): region for region, locations in region_mapping.items() for loc in locations}

# Apply NER to classify regions
data['primary_region'] = data['article'].apply(extract_regions)

# Save the updated dataset
data.to_csv("processed_testdataset.csv", index=False)
print("Processed dataset saved to processed_testdataset.csv")

"""NER - entities " regions" labeling

vectorize - TF-IDF
"""

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score
from sklearn.model_selection import train_test_split

# Set paths for your datasets
train_path = "processed_dataset.csv"
test_path = "processed_testdataset.csv"

# Load datasets
train_data = pd.read_csv(train_path)
test_data = pd.read_csv(test_path)

# Check data structure
print("Train Data Columns:", train_data.columns)
print("Test Data Columns:", test_data.columns)

# Initialize TF-IDF Vectorizer
vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')

# Fit the vectorizer on the training data and transform
X_train = vectorizer.fit_transform(train_data['article'])
X_test = vectorizer.transform(test_data['article'])

# Extract labels (primary region) as y for training and testing
y_train = train_data['primary_region']
y_test = test_data['primary_region']

print("Number of rows in X_test:", X_test.shape[0])
print("Number of rows in y_test:", len(y_test))

# Drop rows with missing values in both 'preprocessed_article' and 'primary_region'
test_data = test_data.dropna(subset=['article', 'primary_region'])

# Recreate X_test and y_test
X_test = vectorizer.transform(test_data['article'])
y_test = test_data['primary_region']

# Ensure alignment
assert X_test.shape[0] == len(y_test), "Mismatch between X_test and y_test!"

print("Number of NaN values in y_train:", y_train.isna().sum())
print("Number of NaN values in y_test:", y_test.isna().sum())

# Drop rows with NaN labels
train_data = train_data.dropna(subset=['primary_region'])
test_data = test_data.dropna(subset=['primary_region'])

# Update y_train and y_test
y_train = train_data['primary_region']
y_test = test_data['primary_region']

print("Number of rows in train_data:", train_data.shape[0])
print("Number of samples in X_train (TF-IDF matrix):", X_train.shape[0])
print("Number of samples in y_train (labels):", len(y_train))

# Drop rows with NaN in 'primary_region' and synchronize the dataset
train_data = train_data.dropna(subset=['primary_region', 'article'])

# Verify synchronization
print("Number of rows in train_data after drop:", train_data.shape[0])

# Reassign labels
y_train = train_data['primary_region']

# Re-vectorize the text
X_train = vectorizer.fit_transform(train_data['article'])

"""classifier - LR"""

# Initialize Logistic Regression Classifier
classifier = LogisticRegression(max_iter=1000, random_state=42)

# Train the classifier
classifier.fit(X_train, y_train)

# Predict on the test set
y_pred = classifier.predict(X_test)

# Generate classification report and accuracy score
print("Classification Report:\n", classification_report(y_test, y_pred))
print("Accuracy Score:", accuracy_score(y_test, y_pred))

# Simulate multiple epochs (retraining and evaluating multiple times)
num_epochs = 10
for epoch in range(num_epochs):
    # Re-train the model
    classifier.fit(X_train, y_train)

    # Predict on the test set
    y_pred = classifier.predict(X_test)

    # Print accuracy for each epoch
    print(f"Epoch {epoch + 1}/{num_epochs}")
    print("Accuracy Score:", accuracy_score(y_test, y_pred))
    print("Classification Report:\n", classification_report(y_test, y_pred))

"""fine tuning LR

"""

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report, accuracy_score

# Define the parameter grid for Logistic Regression
param_grid = {
    'C': [0.01, 0.1, 1, 10],         # Regularization strength
    'penalty': ['l2', 'l1', 'none'], # Type of regularization
    'solver': ['liblinear', 'saga']  # Optimization algorithms
}

# Initialize the Logistic Regression model
lr_classifier = LogisticRegression(max_iter=1000, random_state=42)

# Initialize GridSearchCV
grid_search = GridSearchCV(
    estimator=lr_classifier,
    param_grid=param_grid,
    scoring='accuracy',
    cv=3,  # 3-fold cross-validation
    verbose=1,
    n_jobs=-1  # Use all available cores
)

# Fit the GridSearchCV to find the best hyperparameters
grid_search.fit(X_train, y_train)

# Get the best parameters and score
best_params = grid_search.best_params_
best_score = grid_search.best_score_

print("Best Parameters:", best_params)
print("Best Cross-Validation Accuracy:", best_score)

# Train the final model with the best parameters
final_lr_classifier = grid_search.best_estimator_

# Predict on the test set
y_pred = final_lr_classifier.predict(X_test)

# Evaluate the final model
print("Final Model Classification Report:\n", classification_report(y_test, y_pred))
print("Final Model Accuracy Score:", accuracy_score(y_test, y_pred))

"""svm"""

from sklearn.svm import LinearSVC
from sklearn.metrics import classification_report, accuracy_score

# Initialize and train the SVM classifier
svm_classifier = LinearSVC(max_iter=1000, random_state=42)
svm_classifier.fit(X_train, y_train)

# Predict on the test set
y_pred_svm = svm_classifier.predict(X_test)

# Print classification report and accuracy for SVM
print("SVM Classifier - Classification Report:\n", classification_report(y_test, y_pred_svm))
print("SVM Classifier - Accuracy Score:", accuracy_score(y_test, y_pred_svm))

"""fine tune svm

"""

from sklearn.model_selection import RandomizedSearchCV
import numpy as np

# Define the parameter distribution for SVM (LinearSVC)
param_dist = {
    'C': np.logspace(-3, 3, 7),    # Sampling from log scale values of C
    'penalty': ['l2', 'none'],
    'max_iter': [1000, 2000, 5000],
}

# Initialize RandomizedSearchCV
random_search = RandomizedSearchCV(
    estimator=svm_classifier,
    param_distributions=param_dist,
    n_iter=20,                     # Number of random combinations to try
    scoring='accuracy',
    cv=3,
    verbose=1,
    n_jobs=-1,
    random_state=42
)

# Fit RandomizedSearchCV to find the best hyperparameters
random_search.fit(X_train, y_train)

# Get the best parameters and score
best_params = random_search.best_params_
best_score = random_search.best_score_

print("Best Parameters (RandomizedSearchCV):", best_params)
print("Best Cross-Validation Accuracy (RandomizedSearchCV):", best_score)

# Train the final model with the best parameters
final_svm_classifier = random_search.best_estimator_

# Predict and evaluate
y_pred_svm = final_svm_classifier.predict(X_test)
print("SVM Classifier - Final Classification Report:\n", classification_report(y_test, y_pred_svm))
print("SVM Classifier - Final Accuracy Score:", accuracy_score(y_test, y_pred_svm))

"""xgboost"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Ensure no missing values in 'primary_region' or 'preprocessed_article'
train_data = train_data.dropna(subset=['primary_region', 'article'])

# Assign labels
y_train = train_data['primary_region']

# TF-IDF Vectorization
vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')
X_train = vectorizer.fit_transform(train_data['article'])

from sklearn.preprocessing import LabelEncoder

# Initialize LabelEncoder
label_encoder = LabelEncoder()

# Fit and transform y_train
y_train_encoded = label_encoder.fit_transform(y_train)

# Transform y_test
y_test_encoded = label_encoder.transform(y_test)

# Check the mapping
print("Class Mapping:", dict(zip(label_encoder.classes_, range(len(label_encoder.classes_)))))

from xgboost import XGBClassifier
from sklearn.metrics import classification_report, accuracy_score

# Initialize the XGBoost classifier
xgb_classifier = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)

# Train the model
xgb_classifier.fit(X_train, y_train_encoded)

# Predict on the test set
y_pred_xgb_encoded = xgb_classifier.predict(X_test)

# Decode predictions back to original labels
y_pred_xgb = label_encoder.inverse_transform(y_pred_xgb_encoded)

# Print classification report and accuracy
print("XGBoost Classifier - Classification Report:\n", classification_report(y_test, y_pred_xgb))
print("XGBoost Classifier - Accuracy Score:", accuracy_score(y_test, y_pred_xgb))

"""fine tuning xgb"""

from sklearn.model_selection import GridSearchCV
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, accuracy_score

# Define the parameter grid
param_grid = {
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7],
    'n_estimators': [50, 100, 200],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}

# Initialize the XGBoost classifier
xgb_classifier = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)

# Initialize GridSearchCV
grid_search = GridSearchCV(
    estimator=xgb_classifier,
    param_grid=param_grid,
    scoring='accuracy',
    cv=3,  # 3-fold cross-validation
    verbose=1,
    n_jobs=-1  # Use all available cores
)

# Fit GridSearchCV
grid_search.fit(X_train, y_train_encoded)

# Get the best parameters and the best score
best_params = grid_search.best_params_
best_score = grid_search.best_score_

print("Best Parameters:", best_params)
print("Best Cross-Validation Accuracy:", best_score)

# Train the final model with the best parameters
final_model = grid_search.best_estimator_

# Predict on the test set
y_pred_xgb_encoded = final_model.predict(X_test)

# Decode predictions back to original labels
y_pred_xgb = label_encoder.inverse_transform(y_pred_xgb_encoded)

# Evaluate performance
print("Classification Report:\n", classification_report(y_test, y_pred_xgb))
print("Accuracy Score:", accuracy_score(y_test, y_pred_xgb))





"""save the models

"""

import joblib

# Save Logistic Regression model
joblib.dump(classifier, 'logistic_regression_model.pkl')

# Save SVM model
joblib.dump(svm_classifier, 'svm_model.pkl')

# Save XGBoost model
joblib.dump(xgb_classifier, 'xgboost_model.pkl')

print("Models saved successfully!")

import spacy

# Load the SpaCy model
nlp = spacy.load("en_core_web_sm")

# Example text from your dataset
example_text = "Barack Obama was born in Hawaii and served as the President of the United States."

# Apply NER
doc = nlp(example_text)

# Print Named Entities
for ent in doc.ents:
    print(f"Entity: {ent.text}, Label: {ent.label_}")

# Load your dataset
import pandas as pd

dataset_path = "new_test_5k.csv"  # Update with your dataset path
data = pd.read_csv(dataset_path)

# Select one row
example_row = data.iloc[0]
article_text = example_row['article']  # Assuming your dataset has an 'article' column

# Apply NER
doc = nlp(article_text)
print("Named Entities in the Article:")
for ent in doc.ents:
    print(f"Entity: {ent.text}, Label: {ent.label_}")

# Use the classifier to predict the region for this article
vectorized_text = vectorizer.transform([article_text])  # Transform the article into vectorized form
predicted_region = classifier.predict(vectorized_text)
print("\nPredicted Region:", predicted_region[0])





"""#TextRank"""

import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# Load the classified dataset
dataset_path = "processed_testdataset.csv"
data = pd.read_csv(dataset_path)

# Function for TextRank summarization
def textrank_summary(article, num_sentences=3):
    # Split the article into sentences
    sentences = article.split('. ')
    if len(sentences) <= num_sentences:
        return article  # Return the full article if too short

    # Create a count vectorizer
    vectorizer = CountVectorizer().fit_transform(sentences)
    similarity_matrix = cosine_similarity(vectorizer)

    # Rank sentences based on their similarity scores
    scores = similarity_matrix.sum(axis=1)
    ranked_sentences = [sentences[i] for i in np.argsort(scores)[::-1]]

    # Return top-ranked sentences as the summary
    return '. '.join(ranked_sentences[:num_sentences])

# Apply TextRank summarization to each article
data['textrank_summary'] = data['article'].apply(textrank_summary)

# Save the dataset with summaries
data.to_csv("textrank_summarized_dataset.csv", index=False)
print("TextRank summaries saved to textrank_summarized_dataset.csv")

"""#rouge-textrank"""

pip install rouge

pip install rouge-score

from rouge import Rouge

# Initialize ROUGE
rouge = Rouge()

# Calculate ROUGE scores for summaries
data['rouge_scores_textrank'] = data.apply(lambda row: rouge.get_scores(row['textrank_summary'], row['article'], avg=True), axis=1)

# Save ROUGE scores
data.to_csv("summary_with_rouge_scores_textrank.csv", index=False)
print("ROUGE scores saved to summary_with_rouge_scores_textrank.csv")

"""#bleu-textrank"""

from nltk.translate.bleu_score import sentence_bleu

# Calculate BLEU scores for summaries
data['bleu_scores_textrank'] = data.apply(lambda row: sentence_bleu([row['article'].split()], row['textrank_summary'].split()), axis=1)

# Save BLEU scores
data.to_csv("summary_with_bleu_scores_textrank.csv", index=False)
print("BLEU scores saved to summary_with_bleu_scores_textrank.csv")

"""# LexRank"""

pip install nltk

pip install lexrank

from lexrank import LexRank
from lexrank.mappings.stopwords import STOPWORDS
from nltk.tokenize import sent_tokenize
import pandas as pd
import nltk
nltk.download('punkt_tab')
# Load the classified dataset
dataset_path = "processed_testdataset.csv"
data = pd.read_csv(dataset_path)

# Use all articles in the dataset to initialize LexRank
corpus = data['article'].dropna().tolist()  # Drop NaN values and convert to list
tokenized_corpus = [sent_tokenize(doc) for doc in corpus]  # Tokenize each document into sentences

# Initialize LexRank with the corpus
lxr = LexRank(tokenized_corpus, stopwords=STOPWORDS['en'])

# Function for LexRank summarization
def lexrank_summary(article, num_sentences=3):
    # Split article into sentences using nltk
    sentences = sent_tokenize(article)
    if len(sentences) <= num_sentences:
        return article  # Return full article if too short

    # Generate LexRank summary
    summary = lxr.get_summary(sentences, summary_size=num_sentences)
    return ' '.join(summary)

# Apply LexRank summarization to each article
data['lexrank_summary'] = data['article'].apply(lexrank_summary)

# Save the dataset with summaries
data.to_csv("lexrank_summarized_dataset.csv", index=False)
print("LexRank summaries saved to lexrank_summarized_dataset.csv")

"""## rouge-lexrank"""

from rouge import Rouge

# Initialize ROUGE
rouge = Rouge()

# Calculate ROUGE scores for summaries
data['rouge_scores_lexrank'] = data.apply(lambda row: rouge.get_scores(row['lexrank_summary'], row['article'], avg=True), axis=1)

# Save ROUGE scores
data.to_csv("summary_with_rouge_scores_lexrank.csv", index=False)
print("ROUGE scores saved to summary_with_rouge_scores_lexrank.csv")

"""#bleu-lexrank"""

from nltk.translate.bleu_score import sentence_bleu

# Calculate BLEU scores for summaries
data['bleu_scores_lexrank'] = data.apply(lambda row: sentence_bleu([row['article'].split()], row['lexrank_summary'].split()), axis=1)

# Save BLEU scores
data.to_csv("summary_with_bleu_scores_lexrank.csv", index=False)
print("BLEU scores saved to summary_with_bleu_scores_lexrank.csv")

"""## BERTsum"""

pip install transformers

from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

# Load the BERTSum model and tokenizer
model_name = "bert-base-uncased"  # Replace with an actual pre-trained extractive summarization model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

# Function for BERTSum summarization
def bertsum_summary(article, num_sentences=3):
    # Split article into sentences
    sentences = article.split('. ')
    if len(sentences) <= num_sentences:
        return article  # Return the full article if too short

    # Encode sentences using tokenizer
    inputs = tokenizer(sentences, return_tensors="pt", padding=True, truncation=True, max_length=512)

    # Predict importance scores
    with torch.no_grad():
        outputs = model(**inputs)

    # Extract scores and rank sentences
    scores = outputs.logits[:, 1].numpy()
    ranked_sentences = [sentences[i] for i in np.argsort(scores)[::-1]]

    # Return top-ranked sentences as the summary
    return '. '.join(ranked_sentences[:num_sentences])

# Apply BERTSum summarization to each article
data['bertsum_summary'] = data['article'].apply(bertsum_summary)

# Save the dataset with summaries
data.to_csv("bertsum_summarized_dataset.csv", index=False)
print("BERTSum summaries saved to bertsum_summarized_dataset.csv")

import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import numpy as np

# Load processed test dataset
test_dataset_path = "processed_testdataset.csv"  # Replace with the correct path
data = pd.read_csv(test_dataset_path)

# Select only the first 5 rows of the dataset
data = data.head(5)

# Load the BERTSum model and tokenizer
model_name = "bert-base-uncased"  # Replace with a fine-tuned BERTSum model name if available
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

# Function for BERTSum summarization
def bertsum_summary(article, num_sentences=3):
    # Split article into sentences
    sentences = article.split('. ')
    if len(sentences) <= num_sentences:
        return article  # Return the full article if too short

    # Encode sentences using tokenizer
    inputs = tokenizer(sentences, return_tensors="pt", padding=True, truncation=True, max_length=512)

    # Predict importance scores
    with torch.no_grad():
        outputs = model(**inputs)

    # Extract scores and rank sentences
    scores = outputs.logits[:, 1].numpy()
    ranked_sentences = [sentences[i] for i in np.argsort(scores)[::-1]]

    # Return top-ranked sentences as the summary
    return '. '.join(ranked_sentences[:num_sentences])

# Apply BERTSum summarization to each article
data['bertsum_summary'] = data['article'].apply(bertsum_summary)

# Save the dataset with summaries
output_path = "bertsum_summarized_first5.csv"
data.to_csv(output_path, index=False)

output_path

"""#rouge-bertsum"""

from rouge import Rouge

# Initialize ROUGE
rouge = Rouge()

# Calculate ROUGE scores for summaries
data['rouge_scores_bertsum'] = data.apply(lambda row: rouge.get_scores(row['bertsum_summary'], row['article'], avg=True), axis=1)

# Save ROUGE scores
data.to_csv("summary_with_rouge_scores_bertsum.csv", index=False)
print("ROUGE scores saved to summary_with_rouge_scores_bertsum.csv")

"""bleu-bertsum"""

from nltk.translate.bleu_score import sentence_bleu

# Calculate BLEU scores for summaries
data['bleu_scores_bertsum'] = data.apply(lambda row: sentence_bleu([row['article'].split()], row['bertsum_summary'].split()), axis=1)

# Save BLEU scores
data.to_csv("summary_with_bleu_scores_bertsum.csv", index=False)
print("BLEU scores saved to summary_with_bleu_scores_bertsum.csv")

"""#T5"""

import pandas as pd
from transformers import T5Tokenizer, T5ForConditionalGeneration

# Load your dataset
dataset_path = "processed_testdataset.csv"  # Replace with the correct path
data = pd.read_csv(test_dataset_path)

data = data.head(5)

# Load T5 model and tokenizer
model_name = "t5-small"  # You can use 't5-base' or 't5-large' for larger models
tokenizer = T5Tokenizer.from_pretrained(model_name)
model = T5ForConditionalGeneration.from_pretrained(model_name)

# Define the summarization function
def t5_summarize(article):
    inputs = tokenizer.encode("summarize: " + article, return_tensors="pt", max_length=512, truncation=True)
    summary_ids = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)
    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)

# Apply summarization to each article
data['t5_summary'] = data['article'].apply(t5_summarize)

# Save the summarized dataset
summarized_dataset_path = "t5_summarized_dataset.csv"
data.to_csv(summarized_dataset_path, index=False)

print(f"Summarized dataset saved to {summarized_dataset_path}")

"""#t5- rouge"""

from rouge import Rouge

# Initialize ROUGE
rouge = Rouge()

# Calculate ROUGE scores for summaries
data['rouge_scores_t5'] = data.apply(lambda row: rouge.get_scores(row['t5_summary'], row['article'], avg=True), axis=1)

# Save ROUGE scores
data.to_csv("t5_summarized_dataset.csv", index=False)
print("ROUGE scores saved to summary_with_rouge_scores_t5.csv")

"""#t-5 bleu score"""

from nltk.translate.bleu_score import sentence_bleu

# Calculate BLEU scores for summaries
data['bleu_scores_'] = data.apply(lambda row: sentence_bleu([row['article'].split()], row['t5_summary'].split()), axis=1)

# Save BLEU scores
data.to_csv("t5_summarized_dataset.csv", index=False)
print("BLEU scores saved to summary_with_bleu_scores_t5.csv")

"""# Bart"""

import pandas as pd
import joblib
from transformers import BartForConditionalGeneration, BartTokenizer

# Load the dataset (pre-classified with 'primary_region' column)
dataset_path = "processed_testdataset.csv"
data = pd.read_csv(dataset_path)

data = data.head(5)

# OPTIONAL: Load your classification model if reclassification is needed
classifier = joblib.load("logistic_regression_model.pkl")# Load the BART model and tokenizer
model_name = "facebook/bart-large-cnn"
tokenizer = BartTokenizer.from_pretrained(model_name)
model = BartForConditionalGeneration.from_pretrained(model_name)

# Function to generate a summary for a single article
def generate_summary(article_text):
    # Tokenize and encode the input
    inputs = tokenizer.encode("summarize: " + article_text, return_tensors="pt", max_length=1024, truncation=True)

    # Generate the summary
    summary_ids = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)

    # Decode the output
    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)

# Apply summarization to each article in the dataset
data['summary'] = data['article'].apply(generate_summary)

# Save the summarized dataset
summarized_dataset_path = "Bart_model.csv"
data.to_csv(summarized_dataset_path, index=False)

print(f"Summarized dataset saved to {summarized_dataset_path}")

"""# BART - ROUGE"""

from rouge import Rouge

# Initialize ROUGE
rouge = Rouge()

# Calculate ROUGE scores for summaries
data['rouge_scores_bart'] = data.apply(lambda row: rouge.get_scores(row['summary'], row['article'], avg=True), axis=1)

# Save ROUGE scores
data.to_csv("Bart_model.csv", index=False)
print("ROUGE scores saved to summary_with_rouge_scores_bart.csv")

"""# bart bleu"""

from nltk.translate.bleu_score import sentence_bleu

# Calculate BLEU scores for summaries
data['bleu_scores_'] = data.apply(lambda row: sentence_bleu([row['article'].split()], row['summary'].split()), axis=1)

# Save BLEU scores
data.to_csv("Bart_model.csv", index=False)
print("BLEU scores saved to summary_with_bleu_scores_bart.csv")

"""#pegasus"""

import pandas as pd
from transformers import PegasusTokenizer, PegasusForConditionalGeneration
import torch

# Load processed test dataset
test_dataset_path = "processed_testdataset.csv"  # Replace with the correct path to your dataset
data = pd.read_csv(test_dataset_path)

# Select only the first 5 rows of the dataset
data = data.head(5)

# Load Pegasus model and tokenizer
model_name = "google/pegasus-xsum"  # Use a Pegasus model pre-trained for summarization tasks
tokenizer = PegasusTokenizer.from_pretrained(model_name)
model = PegasusForConditionalGeneration.from_pretrained(model_name)

# Function for Pegasus summarization
def pegasus_summary(article, max_length=60, min_length=20):
    # Encode the input article
    inputs = tokenizer(article, return_tensors="pt", max_length=512, truncation=True)

    # Generate summary
    with torch.no_grad():
        summary_ids = model.generate(
            inputs["input_ids"],
            max_length=max_length,
            min_length=min_length,
            length_penalty=2.0,
            num_beams=4,
            early_stopping=True
        )
    # Decode and return the summary
    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)

# Apply Pegasus summarization to each article
data['pegasus_summary'] = data['article'].apply(pegasus_summary)

# Save the dataset with summaries
output_path = "pegasus_summarized_first5.csv"
data.to_csv(output_path, index=False)

print(f"Pegasus summaries saved to {output_path}")

"""#pegasus rouge"""

from rouge import Rouge

# Initialize ROUGE
rouge = Rouge()

# Calculate ROUGE scores for summaries
data['rouge_scores_pegasus'] = data.apply(lambda row: rouge.get_scores(row['pegasus_summary'], row['article'], avg=True), axis=1)

# Save ROUGE scores
data.to_csv("pegasus_summarized_first5.csv", index=False)
print("ROUGE scores saved to summary_with_rouge_scores_pegasus.csv")

"""# pegasus - bleu"""

from nltk.translate.bleu_score import sentence_bleu

# Calculate BLEU scores for summaries
data['bleu_scores_pegasus'] = data.apply(lambda row: sentence_bleu([row['article'].split()], row['pegasus_summary'].split()), axis=1)

# Save BLEU scores
data.to_csv("pegasus_summarized_first5.csv", index=False)
print("BLEU scores saved to summary_with_bleu_scores_pegasus.csv")

