{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOBexW_l0zaI"
      },
      "outputs": [],
      "source": [
        "pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOzZuLFr2RM9"
      },
      "source": [
        "## TextRank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2Hz7_0V2W66"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Load the classified dataset\n",
        "dataset_path = \"processed_testdataset.csv\"\n",
        "data = pd.read_csv(dataset_path)\n",
        "\n",
        "# Function for TextRank summarization\n",
        "def textrank_summary(article, num_sentences=3):\n",
        "    # Split the article into sentences\n",
        "    sentences = article.split('. ')\n",
        "    if len(sentences) <= num_sentences:\n",
        "        return article  # Return the full article if too short\n",
        "\n",
        "    # Create a count vectorizer\n",
        "    vectorizer = CountVectorizer().fit_transform(sentences)\n",
        "    similarity_matrix = cosine_similarity(vectorizer)\n",
        "\n",
        "    # Rank sentences based on their similarity scores\n",
        "    scores = similarity_matrix.sum(axis=1)\n",
        "    ranked_sentences = [sentences[i] for i in np.argsort(scores)[::-1]]\n",
        "\n",
        "    # Return top-ranked sentences as the summary\n",
        "    return '. '.join(ranked_sentences[:num_sentences])\n",
        "\n",
        "# Apply TextRank summarization to each article\n",
        "data['textrank_summary'] = data['article'].apply(textrank_summary)\n",
        "\n",
        "# Save the dataset with summaries\n",
        "data.to_csv(\"textrank_summarized_dataset.csv\", index=False)\n",
        "print(\"TextRank summaries saved to textrank_summarized_dataset.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z78MQoH22Z0d"
      },
      "source": [
        "## ROUGE Score-textrank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJDdAZki2e3d"
      },
      "outputs": [],
      "source": [
        "pip install rouge\n",
        "\n",
        "pip install rouge-score\n",
        "\n",
        "from rouge import Rouge\n",
        "\n",
        "# Initialize ROUGE\n",
        "rouge = Rouge()\n",
        "\n",
        "# Calculate ROUGE scores for summaries\n",
        "data['rouge_scores_textrank'] = data.apply(lambda row: rouge.get_scores(row['textrank_summary'], row['article'], avg=True), axis=1)\n",
        "\n",
        "# Save ROUGE scores\n",
        "data.to_csv(\"summary_with_rouge_scores_textrank.csv\", index=False)\n",
        "print(\"ROUGE scores saved to summary_with_rouge_scores_textrank.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xc4bV242l_l"
      },
      "source": [
        "## BLEU SCORE-textrank\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgsvDb_52uZJ"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "# Calculate BLEU scores for summaries\n",
        "data['bleu_scores_textrank'] = data.apply(lambda row: sentence_bleu([row['article'].split()], row['textrank_summary'].split()), axis=1)\n",
        "\n",
        "# Save BLEU scores\n",
        "data.to_csv(\"summary_with_bleu_scores_textrank.csv\", index=False)\n",
        "print(\"BLEU scores saved to summary_with_bleu_scores_textrank.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6vnbxE021SD"
      },
      "source": [
        "## LexRank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o27xsTbc28Jq"
      },
      "outputs": [],
      "source": [
        "pip install nltk\n",
        "\n",
        "pip install lexrank\n",
        "\n",
        "from lexrank import LexRank\n",
        "from lexrank.mappings.stopwords import STOPWORDS\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "# Load the classified dataset\n",
        "dataset_path = \"processed_testdataset.csv\"\n",
        "data = pd.read_csv(dataset_path)\n",
        "\n",
        "# Use all articles in the dataset to initialize LexRank\n",
        "corpus = data['article'].dropna().tolist()  # Drop NaN values and convert to list\n",
        "tokenized_corpus = [sent_tokenize(doc) for doc in corpus]  # Tokenize each document into sentences\n",
        "\n",
        "# Initialize LexRank with the corpus\n",
        "lxr = LexRank(tokenized_corpus, stopwords=STOPWORDS['en'])\n",
        "\n",
        "# Function for LexRank summarization\n",
        "def lexrank_summary(article, num_sentences=3):\n",
        "    # Split article into sentences using nltk\n",
        "    sentences = sent_tokenize(article)\n",
        "    if len(sentences) <= num_sentences:\n",
        "        return article  # Return full article if too short\n",
        "\n",
        "    # Generate LexRank summary\n",
        "    summary = lxr.get_summary(sentences, summary_size=num_sentences)\n",
        "    return ' '.join(summary)\n",
        "\n",
        "# Apply LexRank summarization to each article\n",
        "data['lexrank_summary'] = data['article'].apply(lexrank_summary)\n",
        "\n",
        "# Save the dataset with summaries\n",
        "data.to_csv(\"lexrank_summarized_dataset.csv\", index=False)\n",
        "print(\"LexRank summaries saved to lexrank_summarized_dataset.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFCirXij29p6"
      },
      "source": [
        "## ROUGE Score-lexrank\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ahUgN2J3AGU"
      },
      "outputs": [],
      "source": [
        "from rouge import Rouge\n",
        "\n",
        "# Initialize ROUGE\n",
        "rouge = Rouge()\n",
        "\n",
        "# Calculate ROUGE scores for summaries\n",
        "data['rouge_scores_lexrank'] = data.apply(lambda row: rouge.get_scores(row['lexrank_summary'], row['article'], avg=True), axis=1)\n",
        "\n",
        "# Save ROUGE scores\n",
        "data.to_csv(\"summary_with_rouge_scores_lexrank.csv\", index=False)\n",
        "print(\"ROUGE scores saved to summary_with_rouge_scores_lexrank.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3F87IHrK3Db7"
      },
      "source": [
        "## BLEU SCORE-lexrank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BalqMaW3FSx"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "# Calculate BLEU scores for summaries\n",
        "data['bleu_scores_lexrank'] = data.apply(lambda row: sentence_bleu([row['article'].split()], row['lexrank_summary'].split()), axis=1)\n",
        "\n",
        "# Save BLEU scores\n",
        "data.to_csv(\"summary_with_bleu_scores_lexrank.csv\", index=False)\n",
        "print(\"BLEU scores saved to summary_with_bleu_scores_lexrank.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_B0dSUR3Hyq"
      },
      "source": [
        "## BERTsum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbGZkJ7W3Lbv"
      },
      "outputs": [],
      "source": [
        "pip install transformers\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Load the BERTSum model and tokenizer\n",
        "model_name = \"bert-base-uncased\"  # Replace with an actual pre-trained extractive summarization model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Function for BERTSum summarization\n",
        "def bertsum_summary(article, num_sentences=3):\n",
        "    # Split article into sentences\n",
        "    sentences = article.split('. ')\n",
        "    if len(sentences) <= num_sentences:\n",
        "        return article  # Return the full article if too short\n",
        "\n",
        "    # Encode sentences using tokenizer\n",
        "    inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "\n",
        "    # Predict importance scores\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Extract scores and rank sentences\n",
        "    scores = outputs.logits[:, 1].numpy()\n",
        "    ranked_sentences = [sentences[i] for i in np.argsort(scores)[::-1]]\n",
        "\n",
        "    # Return top-ranked sentences as the summary\n",
        "    return '. '.join(ranked_sentences[:num_sentences])\n",
        "\n",
        "# Apply BERTSum summarization to each article\n",
        "data['bertsum_summary'] = data['article'].apply(bertsum_summary)\n",
        "\n",
        "# Save the dataset with summaries\n",
        "data.to_csv(\"bertsum_summarized_dataset.csv\", index=False)\n",
        "print(\"BERTSum summaries saved to bertsum_summarized_dataset.csv\")\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import numpy as np\n",
        "\n",
        "# Load processed test dataset\n",
        "test_dataset_path = \"processed_testdataset.csv\"  # Replace with the correct path\n",
        "data = pd.read_csv(test_dataset_path)\n",
        "\n",
        "# Select only the first 5 rows of the dataset\n",
        "data = data.head(5)\n",
        "\n",
        "# Load the BERTSum model and tokenizer\n",
        "model_name = \"bert-base-uncased\"  # Replace with a fine-tuned BERTSum model name if available\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Function for BERTSum summarization\n",
        "def bertsum_summary(article, num_sentences=3):\n",
        "    # Split article into sentences\n",
        "    sentences = article.split('. ')\n",
        "    if len(sentences) <= num_sentences:\n",
        "        return article  # Return the full article if too short\n",
        "\n",
        "    # Encode sentences using tokenizer\n",
        "    inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "\n",
        "    # Predict importance scores\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Extract scores and rank sentences\n",
        "    scores = outputs.logits[:, 1].numpy()\n",
        "    ranked_sentences = [sentences[i] for i in np.argsort(scores)[::-1]]\n",
        "\n",
        "    # Return top-ranked sentences as the summary\n",
        "    return '. '.join(ranked_sentences[:num_sentences])\n",
        "\n",
        "# Apply BERTSum summarization to each article\n",
        "data['bertsum_summary'] = data['article'].apply(bertsum_summary)\n",
        "\n",
        "# Save the dataset with summaries\n",
        "output_path = \"bertsum_summarized_first5.csv\"\n",
        "data.to_csv(output_path, index=False)\n",
        "\n",
        "output_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ySa79p23bER"
      },
      "source": [
        "## ROUGE Score-bertsum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n69VH_Ct3dqC"
      },
      "outputs": [],
      "source": [
        "from rouge import Rouge\n",
        "\n",
        "# Initialize ROUGE\n",
        "rouge = Rouge()\n",
        "\n",
        "# Calculate ROUGE scores for summaries\n",
        "data['rouge_scores_bertsum'] = data.apply(lambda row: rouge.get_scores(row['bertsum_summary'], row['article'], avg=True), axis=1)\n",
        "\n",
        "# Save ROUGE scores\n",
        "data.to_csv(\"summary_with_rouge_scores_bertsum.csv\", index=False)\n",
        "print(\"ROUGE scores saved to summary_with_rouge_scores_bertsum.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oqlHRmM3hlV"
      },
      "source": [
        "## BLEU SCORE-bertsum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNn2b1rK3lhW"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "# Calculate BLEU scores for summaries\n",
        "data['bleu_scores_bertsum'] = data.apply(lambda row: sentence_bleu([row['article'].split()], row['bertsum_summary'].split()), axis=1)\n",
        "\n",
        "# Save BLEU scores\n",
        "data.to_csv(\"summary_with_bleu_scores_bertsum.csv\", index=False)\n",
        "print(\"BLEU scores saved to summary_with_bleu_scores_bertsum.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
